{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.types import Integer, Date, Time,Text\n",
    "\n",
    "import os\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from prefect import flow, task\n",
    "from prefect.tasks import task_input_hash\n",
    "from prefect_sqlalchemy import SqlAlchemyConnector\n",
    "\n",
    "import pprint\n",
    "from metabase import Metabase\n",
    "from metabase_api import Metabase_API\n",
    "\n",
    "import pipeline_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=DOWNLOAD'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_set.url_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=DOWNLOAD'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(pipeline_set, \"url_C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(data_type):\n",
    "    if data_type == \"check\":\n",
    "        return(\"check\", data_type)\n",
    "    elif data_type == \"metabase\":\n",
    "        return(\"metabase\", data_type)\n",
    "    elif data_type[:1] in [\"C\",\"V\",\"P\"] and data_type[1:] == \" reload\":\n",
    "        data_type = data_type[:1]\n",
    "        url = getattr(pipeline_set,f\"url_{data_type}\")\n",
    "        csv_name = f\"MVC_{data_type}.csv\"\n",
    "        os.system(f\"wget {url} -O {csv_name}\")\n",
    "        return(csv_name, data_type)\n",
    "    elif data_type in [\"C\",\"V\",\"P\"]:\n",
    "        url = getattr(pipeline_set,f\"url_{data_type}\")\n",
    "        csv_name = f\"MVC_{data_type}.csv\"\n",
    "        if os.path.isfile(csv_name) is not True:\n",
    "            os.system(f\"wget {url} -O {csv_name}\")\n",
    "            return(csv_name, data_type)\n",
    "        else:\n",
    "            return(csv_name, data_type)\n",
    "    else:\n",
    "        return(\"err\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MVC_P.csv', 'P')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type = 'V'\n",
    "download_data(data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables(csv_name:str, years:list , data_type:str):\n",
    "    # Reads the csv\n",
    "    df = pd.read_csv(csv_name, nrows = 1000, low_memory=False)\n",
    "    # Selects specified columns\n",
    "    df = df[getattr(pipeline_set,f\"sel_{data_type}\")]\n",
    "    # Rename them \n",
    "    df.rename(columns=(getattr(pipeline_set,f\"sel_rename_{data_type}\")),inplace=True)\n",
    "    # Convert to datetime type in datetime columns\n",
    "    df.crash_date = pd.to_datetime(df.crash_date).dt.date\n",
    "    df.crash_time = pd.to_datetime(df.crash_time,format= '%H:%M' ).dt.time\n",
    "\n",
    "    # Creates empty tables in a database\n",
    "    engine = create_engine('postgresql://root:root@localhost:5432/MVC_db') \n",
    "    \n",
    "    for i in years:\n",
    "        df.head(n=0).to_sql(name = f\"MVC_{data_type}_{i}\",con = engine, dtype=(getattr(pipeline_set,f\"sel_types_{data_type}\")),if_exists = 'replace')\n",
    "\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(postgresql://root:***@localhost:5432/MVC_db)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_name = 'MVC_P.csv'\n",
    "years = [i for i in range(2017,2024)]\n",
    "data_type = 'P'\n",
    "\n",
    "create_tables(csv_name, years, data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_load(years:list, csv_name:str, engine, data_type:str):\n",
    "    \n",
    "    total_rows=0\n",
    "    total_rows_loaded = 0\n",
    "    total_time = 0\n",
    "\n",
    "    # Iterates through chunks of data from the CSV\n",
    "    df_iter = pd.read_csv(csv_name, iterator = True, chunksize = 100000, low_memory=False)\n",
    "    df = next(df_iter)\n",
    "    \n",
    "    while len(df) > 0:\n",
    "        try:\n",
    "            start_time = time()\n",
    "            \n",
    "            # Process data\n",
    "            df = df[getattr(pipeline_set,f\"sel_{data_type}\")]\n",
    "            df.rename(columns=(getattr(pipeline_set,f\"sel_rename_{data_type}\")),inplace=True)\n",
    "            df.crash_date = pd.to_datetime(df.crash_date).dt.date\n",
    "            df.crash_time = pd.to_datetime(df.crash_time,format= '%H:%M' ).dt.time\n",
    "            \n",
    "            total_rows += len(df)\n",
    "            \n",
    "            # Appends them to the tables\n",
    "            for i in years:\n",
    "                df_temp = df.loc[pd.DatetimeIndex(df.crash_date).year == i]\n",
    "                df_temp.to_sql(name = f\"MVC_{data_type}_{i}\",con = engine, if_exists = 'append')\n",
    "                \n",
    "                total_rows_loaded += len(df_temp)\n",
    "            \n",
    "\n",
    "            end_time = time()\n",
    "            total_time += (end_time - start_time)\n",
    "\n",
    "\n",
    "            print(\n",
    "            f\"total rows processed = {total_rows}\", \n",
    "            f\"total rows loaded = {total_rows_loaded}\", \n",
    "            'iteration took %.2f seconds' % (end_time - start_time),\n",
    "            f\"total time = %.2f seconds\" % (total_time),\n",
    "            \"\", sep = \"\\n\")\n",
    "\n",
    "            df = next(df_iter)\n",
    "\n",
    "        except StopIteration:\n",
    "            print(\"Finished ingesting data into the postgres database\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows processed = 100000\n",
      "total rows loaded = 100000\n",
      "iteration took 14.25 seconds\n",
      "total time = 14.25 seconds\n",
      "\n",
      "total rows processed = 200000\n",
      "total rows loaded = 200000\n",
      "iteration took 13.16 seconds\n",
      "total time = 27.41 seconds\n",
      "\n",
      "total rows processed = 300000\n",
      "total rows loaded = 300000\n",
      "iteration took 12.70 seconds\n",
      "total time = 40.11 seconds\n",
      "\n",
      "total rows processed = 400000\n",
      "total rows loaded = 400000\n",
      "iteration took 12.64 seconds\n",
      "total time = 52.75 seconds\n",
      "\n",
      "total rows processed = 500000\n",
      "total rows loaded = 500000\n",
      "iteration took 12.83 seconds\n",
      "total time = 65.59 seconds\n",
      "\n",
      "total rows processed = 600000\n",
      "total rows loaded = 600000\n",
      "iteration took 13.51 seconds\n",
      "total time = 79.10 seconds\n",
      "\n",
      "total rows processed = 700000\n",
      "total rows loaded = 700000\n",
      "iteration took 13.34 seconds\n",
      "total time = 92.44 seconds\n",
      "\n",
      "total rows processed = 800000\n",
      "total rows loaded = 800000\n",
      "iteration took 12.28 seconds\n",
      "total time = 104.73 seconds\n",
      "\n",
      "total rows processed = 900000\n",
      "total rows loaded = 900000\n",
      "iteration took 13.16 seconds\n",
      "total time = 117.89 seconds\n",
      "\n",
      "total rows processed = 1000000\n",
      "total rows loaded = 1000000\n",
      "iteration took 13.91 seconds\n",
      "total time = 131.80 seconds\n",
      "\n",
      "total rows processed = 1100000\n",
      "total rows loaded = 1100000\n",
      "iteration took 13.14 seconds\n",
      "total time = 144.93 seconds\n",
      "\n",
      "total rows processed = 1200000\n",
      "total rows loaded = 1200000\n",
      "iteration took 14.06 seconds\n",
      "total time = 158.99 seconds\n",
      "\n",
      "total rows processed = 1300000\n",
      "total rows loaded = 1300000\n",
      "iteration took 11.98 seconds\n",
      "total time = 170.97 seconds\n",
      "\n",
      "total rows processed = 1400000\n",
      "total rows loaded = 1400000\n",
      "iteration took 13.33 seconds\n",
      "total time = 184.30 seconds\n",
      "\n",
      "total rows processed = 1500000\n",
      "total rows loaded = 1500000\n",
      "iteration took 12.84 seconds\n",
      "total time = 197.14 seconds\n",
      "\n",
      "total rows processed = 1600000\n",
      "total rows loaded = 1600000\n",
      "iteration took 12.00 seconds\n",
      "total time = 209.14 seconds\n",
      "\n",
      "total rows processed = 1700000\n",
      "total rows loaded = 1700000\n",
      "iteration took 12.92 seconds\n",
      "total time = 222.06 seconds\n",
      "\n",
      "total rows processed = 1800000\n",
      "total rows loaded = 1800000\n",
      "iteration took 14.67 seconds\n",
      "total time = 236.73 seconds\n",
      "\n",
      "total rows processed = 1900000\n",
      "total rows loaded = 1900000\n",
      "iteration took 19.26 seconds\n",
      "total time = 255.99 seconds\n",
      "\n",
      "total rows processed = 2000000\n",
      "total rows loaded = 2000000\n",
      "iteration took 17.84 seconds\n",
      "total time = 273.83 seconds\n",
      "\n",
      "total rows processed = 2100000\n",
      "total rows loaded = 2100000\n",
      "iteration took 23.35 seconds\n",
      "total time = 297.17 seconds\n",
      "\n",
      "total rows processed = 2200000\n",
      "total rows loaded = 2200000\n",
      "iteration took 18.08 seconds\n",
      "total time = 315.25 seconds\n",
      "\n",
      "total rows processed = 2300000\n",
      "total rows loaded = 2300000\n",
      "iteration took 23.45 seconds\n",
      "total time = 338.70 seconds\n",
      "\n",
      "total rows processed = 2400000\n",
      "total rows loaded = 2400000\n",
      "iteration took 21.65 seconds\n",
      "total time = 360.35 seconds\n",
      "\n",
      "total rows processed = 2500000\n",
      "total rows loaded = 2500000\n",
      "iteration took 20.95 seconds\n",
      "total time = 381.30 seconds\n",
      "\n",
      "total rows processed = 2600000\n",
      "total rows loaded = 2600000\n",
      "iteration took 18.85 seconds\n",
      "total time = 400.16 seconds\n",
      "\n",
      "total rows processed = 2700000\n",
      "total rows loaded = 2700000\n",
      "iteration took 18.28 seconds\n",
      "total time = 418.44 seconds\n",
      "\n",
      "total rows processed = 2800000\n",
      "total rows loaded = 2800000\n",
      "iteration took 20.07 seconds\n",
      "total time = 438.51 seconds\n",
      "\n",
      "total rows processed = 2900000\n",
      "total rows loaded = 2900000\n",
      "iteration took 23.05 seconds\n",
      "total time = 461.56 seconds\n",
      "\n",
      "total rows processed = 3000000\n",
      "total rows loaded = 3000000\n",
      "iteration took 20.92 seconds\n",
      "total time = 482.48 seconds\n",
      "\n",
      "total rows processed = 3100000\n",
      "total rows loaded = 3100000\n",
      "iteration took 19.66 seconds\n",
      "total time = 502.14 seconds\n",
      "\n",
      "total rows processed = 3200000\n",
      "total rows loaded = 3200000\n",
      "iteration took 19.20 seconds\n",
      "total time = 521.33 seconds\n",
      "\n",
      "total rows processed = 3300000\n",
      "total rows loaded = 3300000\n",
      "iteration took 23.60 seconds\n",
      "total time = 544.93 seconds\n",
      "\n",
      "total rows processed = 3400000\n",
      "total rows loaded = 3400000\n",
      "iteration took 19.92 seconds\n",
      "total time = 564.85 seconds\n",
      "\n",
      "total rows processed = 3500000\n",
      "total rows loaded = 3500000\n",
      "iteration took 20.30 seconds\n",
      "total time = 585.15 seconds\n",
      "\n",
      "total rows processed = 3600000\n",
      "total rows loaded = 3600000\n",
      "iteration took 24.70 seconds\n",
      "total time = 609.85 seconds\n",
      "\n",
      "total rows processed = 3700000\n",
      "total rows loaded = 3700000\n",
      "iteration took 24.51 seconds\n",
      "total time = 634.36 seconds\n",
      "\n",
      "total rows processed = 3800000\n",
      "total rows loaded = 3800000\n",
      "iteration took 24.02 seconds\n",
      "total time = 658.38 seconds\n",
      "\n",
      "total rows processed = 3900000\n",
      "total rows loaded = 3900000\n",
      "iteration took 24.32 seconds\n",
      "total time = 682.71 seconds\n",
      "\n",
      "total rows processed = 4000000\n",
      "total rows loaded = 4000000\n",
      "iteration took 17.34 seconds\n",
      "total time = 700.05 seconds\n",
      "\n",
      "total rows processed = 4100000\n",
      "total rows loaded = 4100000\n",
      "iteration took 18.11 seconds\n",
      "total time = 718.16 seconds\n",
      "\n",
      "total rows processed = 4200000\n",
      "total rows loaded = 4200000\n",
      "iteration took 16.68 seconds\n",
      "total time = 734.84 seconds\n",
      "\n",
      "total rows processed = 4300000\n",
      "total rows loaded = 4300000\n",
      "iteration took 18.51 seconds\n",
      "total time = 753.35 seconds\n",
      "\n",
      "total rows processed = 4400000\n",
      "total rows loaded = 4400000\n",
      "iteration took 18.36 seconds\n",
      "total time = 771.72 seconds\n",
      "\n",
      "total rows processed = 4500000\n",
      "total rows loaded = 4500000\n",
      "iteration took 21.08 seconds\n",
      "total time = 792.79 seconds\n",
      "\n",
      "total rows processed = 4600000\n",
      "total rows loaded = 4600000\n",
      "iteration took 16.83 seconds\n",
      "total time = 809.62 seconds\n",
      "\n",
      "total rows processed = 4700000\n",
      "total rows loaded = 4700000\n",
      "iteration took 15.59 seconds\n",
      "total time = 825.21 seconds\n",
      "\n",
      "total rows processed = 4800000\n",
      "total rows loaded = 4800000\n",
      "iteration took 14.40 seconds\n",
      "total time = 839.61 seconds\n",
      "\n",
      "total rows processed = 4900000\n",
      "total rows loaded = 4900000\n",
      "iteration took 15.50 seconds\n",
      "total time = 855.11 seconds\n",
      "\n",
      "total rows processed = 5000000\n",
      "total rows loaded = 5000000\n",
      "iteration took 15.34 seconds\n",
      "total time = 870.45 seconds\n",
      "\n",
      "total rows processed = 5100000\n",
      "total rows loaded = 5100000\n",
      "iteration took 15.33 seconds\n",
      "total time = 885.78 seconds\n",
      "\n",
      "total rows processed = 5165968\n",
      "total rows loaded = 5165968\n",
      "iteration took 9.80 seconds\n",
      "total time = 895.57 seconds\n",
      "\n",
      "Finished ingesting data into the postgres database\n"
     ]
    }
   ],
   "source": [
    "csv_name = 'MVC_P.csv'\n",
    "years = [i for i in range(2012,2024)]\n",
    "data_type = 'P'\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/MVC_db')\n",
    "\n",
    "transform_and_load(years, csv_name, engine, data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_downloaded_data():\n",
    "    engine = create_engine('postgresql://root:root@localhost:5432/MVC_db')\n",
    "\n",
    "    res = [['year', 'Crashes','Vehicles','Person']]\n",
    "    \n",
    "    # Verifies data presence in the database and prints out summaries.\n",
    "    for i in range(2012,2024):\n",
    "        print(f\"{i} spreadsheets check\")\n",
    "        temp = []\n",
    "        temp.append(i)\n",
    "        try:\n",
    "            df = pd.read_sql_query('(SELECT COUNT(*) FROM \"MVC_C_{}\" )'.format(i),con=engine)\n",
    "            t = int(df.get(key = 'count'))\n",
    "        except:\n",
    "            t = 0\n",
    "        temp.append(t)\n",
    "        try:\n",
    "            df = pd.read_sql_query('(SELECT COUNT(*) FROM \"MVC_V_{}\" )'.format(i),con=engine)\n",
    "            t = int(df.get(key = 'count'))\n",
    "        except:\n",
    "            t = 0\n",
    "        temp.append(t)\n",
    "        try:\n",
    "            df = pd.read_sql_query('(SELECT COUNT(*) FROM \"MVC_P_{}\" )'.format(i),con=engine)\n",
    "            t = int(df.get(key = 'count'))\n",
    "        except:\n",
    "            t = 0\n",
    "        temp.append(t)\n",
    "        res.append(temp)\n",
    "    C,V,P =0,0,0\n",
    "    st = '   Downloaded data report:' + '\\n'  + '\\n'\n",
    "    for i in range(13):\n",
    "        for j in range(4):\n",
    "            if i == 0 or j == 0:\n",
    "                h = res[i][j]\n",
    "            else: \n",
    "                h = \"{:,}\".format(res[i][j])\n",
    "            st += str(h).rjust(11)\n",
    "        st += '\\n'\n",
    "        if i > 0:\n",
    "            C += int(res[i][1])\n",
    "            V += int(res[i][2])\n",
    "            P += int(res[i][3])\n",
    "    st = st + '\\n' + 'total'.rjust(11) + str(\"{:,}\".format(C)).rjust(11) + str(\"{:,}\".format(V)).rjust(11) + str(\"{:,}\".format(P)).rjust(11)\n",
    "    print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Crashes'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [['year', 'Crashes','Vehicles','Person']]\n",
    "res[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012 spreadsheets check\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAP14062-local\\AppData\\Local\\Temp\\ipykernel_7100\\354789350.py:13: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  t = int(df.get(key = 'count'))\n",
      "C:\\Users\\LAP14062-local\\AppData\\Local\\Temp\\ipykernel_7100\\354789350.py:19: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  t = int(df.get(key = 'count'))\n",
      "C:\\Users\\LAP14062-local\\AppData\\Local\\Temp\\ipykernel_7100\\354789350.py:25: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
      "  t = int(df.get(key = 'count'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013 spreadsheets check\n",
      "2014 spreadsheets check\n",
      "2015 spreadsheets check\n",
      "2016 spreadsheets check\n",
      "2017 spreadsheets check\n",
      "2018 spreadsheets check\n",
      "2019 spreadsheets check\n",
      "2020 spreadsheets check\n",
      "2021 spreadsheets check\n",
      "2022 spreadsheets check\n",
      "2023 spreadsheets check\n",
      "   Downloaded data report:\n",
      "\n",
      "       year    Crashes   Vehicles     Person\n",
      "       2012    100,545    198,968     27,671\n",
      "       2013    203,740    404,685     55,606\n",
      "       2014    206,033    409,061     51,853\n",
      "       2015    217,694    434,582     52,171\n",
      "       2016    229,831    457,916    800,345\n",
      "       2017    231,007    464,546    961,775\n",
      "       2018    231,564    465,817    946,203\n",
      "       2019    211,486    426,722    854,269\n",
      "       2020    112,916    231,167    413,197\n",
      "       2021    110,548    210,942    386,066\n",
      "       2022    103,875          0    362,532\n",
      "       2023     72,133          0    254,280\n",
      "\n",
      "      total  2,031,372  3,704,406  5,165,968\n"
     ]
    }
   ],
   "source": [
    "check_downloaded_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metabase(years:list):\n",
    "    # Connection creation\n",
    "    mblogin = 'root@gmail.com'\n",
    "    mbpass = 'Aa123456@'\n",
    "    try:\n",
    "        mb = Metabase_API('http://localhost:3001/', mblogin, mbpass)\n",
    "        print(\"connection ok\")\n",
    "    except:\n",
    "        print(\"connection failed\")\n",
    "    \n",
    "    try:\n",
    "        colid = mb.get_item_id('collection', \"MVC_collection\")\n",
    "        print(\"collection 'MVC_collection' exists\")\n",
    "        print(f'colid: {colid}')\n",
    "    except:\n",
    "        print('no collection')\n",
    "    \n",
    "    try:\n",
    "        dbid = mb.get_item_id('database', \"MVC_db\")\n",
    "        print(\"database ok\")\n",
    "        print(f'dbid: {dbid}')\n",
    "    except:\n",
    "        print(\"MVC_db not found:connect or rename database\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection ok\n",
      "collection 'MVC_collection' exists\n",
      "colid: 3\n",
      "database ok\n",
      "dbid: 2\n"
     ]
    }
   ],
   "source": [
    "years = [i for i in range(2012,2024)]\n",
    "metabase(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collection 'MVC_collection' exists\n"
     ]
    }
   ],
   "source": [
    "mblogin = 'root@gmail.com'\n",
    "mbpass = 'Aa123456@'\n",
    "mb = Metabase_API('http://localhost:3001/', mblogin, mbpass)\n",
    "colid = mb.get_item_id('collection', \"MVC_collection\")\n",
    "print(\"collection 'MVC_collection' exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_id = mb.get_item_id('collection', \"MVC_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "BASE_URL = \"http://localhost:3001/\"\n",
    "login_payload = {\n",
    "    \"username\": \"root@gmail.com\",\n",
    "    \"password\": \"Aa123456@\"\n",
    "}\n",
    "\n",
    "response = requests.post(BASE_URL + \"api/session\", json=login_payload)\n",
    "assert response.status_code == 200, \"Failed to log in\"\n",
    "\n",
    "token = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"X-Metabase-Session\": token\n",
    "}\n",
    "\n",
    "response = requests.get(BASE_URL + \"api/collection\", headers=headers)\n",
    "collections = response.json()\n",
    "\n",
    "collection_id = None\n",
    "for collection in collections:\n",
    "    if collection['name'] == \"MVC_collection\":\n",
    "        collection_id = collection['id']\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(BASE_URL + f\"api/collection/{collection_id}/items\", headers=headers)\n",
    "cards_in_collection = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['total', 'data', 'models', 'limit', 'offset'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards_in_collection.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cards_in_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_card = None\n",
    "for card in cards_in_collection['data']:\n",
    "    if card['model'] == 'card' and card['name'] == 'Activity, Count':\n",
    "        target_card = card\n",
    "        break\n",
    "\n",
    "if target_card:\n",
    "    with open('output.json', 'w') as f:\n",
    "        json.dump(target_card, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dash = {\n",
    "    \"description\": null,\n",
    "    \"collection_position\": null,\n",
    "    \"database_id\": null,\n",
    "    \"name\": \"Activity, Count\",\n",
    "    \"moderated_status\": null,\n",
    "    \"fully_parametrized\": true,\n",
    "    \"id\": 1,\n",
    "    \"display\": \"scalar\",\n",
    "    \"entity_id\": \"tk8rsT7qYHhGI3YWqLfu6\",\n",
    "    \"collection_preview\": true,\n",
    "    \"last-edit-info\": {\n",
    "        \"id\": 1,\n",
    "        \"last_name\": null,\n",
    "        \"first_name\": \"root\",\n",
    "        \"email\": \"root@gmail.com\",\n",
    "        \"timestamp\": \"2023-10-08T17:00:04.89381Z\"\n",
    "    },\n",
    "    \"model\": \"card\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
